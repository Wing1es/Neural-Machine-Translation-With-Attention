{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37da31af-5778-4a8b-af6a-7dc51fd5cd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import trax\n",
    "from trax import layers as tl\n",
    "from trax.fastmath import numpy as fastnp\n",
    "from trax.supervised import training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6479a2e-a0dd-4674-a397-e43a4818b5d9",
   "metadata": {},
   "source": [
    "# Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f2276d27-9e4e-493b-ab66-6a4c36d1c431",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(inp, tar):\n",
    "\n",
    "    with open(inp, encoding='utf-8') as f_in, open(tar, encoding='utf-8') as f_ta:\n",
    "\n",
    "        for i_l, t_l in zip(f_in, f_ta):\n",
    "            yield (i_l.strip(), t_l.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1702425d-2b20-4f63-abae-da765362cc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inp_stream(inp, tar):\n",
    "\n",
    "    return lambda _ : generator(inp, tar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ca1fbc0e-9ed2-4d93-814f-2f7085adad76",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stream_fn = inp_stream('data/train.en', 'data/train.de')\n",
    "eval_stream_fn = inp_stream('data/valid.en', 'data/valid.de')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2e42bae6-0640-4569-aab3-293e30729d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"Iran: Gonu's victims, Palestine's crisis, and a stoning suspended · Global Voices\", 'Iran: Gonus Opfer, Krise in Palästina und eine verhinderte Steinigung')\n"
     ]
    }
   ],
   "source": [
    "for example in train_stream_fn(None):\n",
    "    print(example)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "019133d8-d819-40cc-8211-4d9a3d5b51df",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stream = train_stream_fn(None)\n",
    "eval_stream = eval_stream_fn(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3e5c0cce-5ed1-4fbf-93cc-3afdb335aa24",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_FILE = 'ende_32k.subword'\n",
    "VOCAB_DIR = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "63dfa5a1-494a-45b4-b5db-44b156a79f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_train = trax.data.Tokenize(vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)(train_stream)\n",
    "tok_eval = trax.data.Tokenize(vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)(eval_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "71557221-4b0e-4ba8-a357-c4f09f44f88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_eos(stream):\n",
    "\n",
    "    EOS = 1\n",
    "\n",
    "    for (inp, tar) in stream:\n",
    "        inp_eos = list(inp) + [EOS]\n",
    "        tar_eos = list(tar) + [EOS]\n",
    "\n",
    "        yield np.array(inp_eos), np.array(tar_eos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "37692380-68d9-4a82-866c-a2147b90dcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "eos_train = append_eos(tok_train)\n",
    "eos_eval = append_eos(tok_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5f9bc230-065d-40b4-8b71-4ef154b8cc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_train = trax.data.FilterByLength(max_length=512, length_keys=[0,1])(eos_train)\n",
    "len_eval = trax.data.FilterByLength(max_length=512, length_keys=[0,1])(eos_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2fe41550-e012-4121-8bf7-06a6e36aed62",
   "metadata": {},
   "outputs": [],
   "source": [
    "boundaries = [8, 16, 32, 64, 128, 256, 512]\n",
    "batch_size = [256, 128, 64, 32, 16, 8, 4, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "05e0fc21-501f-4733-88e7-0d096e7b6ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = trax.data.BucketByLength(boundaries, batch_size, length_keys=[0,1])(len_train)\n",
    "x_eval = trax.data.BucketByLength(boundaries, batch_size, length_keys=[0,1])(len_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5cb3cb8f-dc77-4234-b5fd-437d7db91f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = trax.data.AddLossWeights(id_to_mask=0)(x_train)\n",
    "x_eval = trax.data.AddLossWeights(id_to_mask=0)(x_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb000b1-97c7-44f0-a86e-14b0f7b476e8",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "149d32db-efeb-4d88-ac82-323746db504c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(inp_str, vocab_file=None, vocab_dir=None):\n",
    "\n",
    "    tok = next(trax.data.Tokenize(iter([inp_str]),vocab_file, vocab_dir))\n",
    "\n",
    "    tok = list(tok) + [1]\n",
    "\n",
    "    bacth_inp = np.reshape(np.array(tok), [1,-1])\n",
    "\n",
    "    return bacth_inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "39cc6ee1-826a-4e62-b214-df86cf90d06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detokenize(integers, vocab_file=None, vocab_dir=None):\n",
    "\n",
    "    tok = list(np.squeeze(integers))\n",
    "\n",
    "    EOS = 1\n",
    "\n",
    "    if EOS in tok:\n",
    "        tok = tok[:tok.index(EOS)]\n",
    "\n",
    "    return trax.data.detokenize(tok, vocab_file=vocab_file, vocab_dir=vocab_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "578d5364-ea07-4187-895f-27f5e208d6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_batch, target_batch, mask_batch = next(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7beb33f9-c2c2-4652-a5cf-b896bdad81ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mTHIS IS THE ENGLISH SENTENCE: \n",
      "\u001b[0m Hugo Miranda of Angel Caido writes that he has always remembered lighting a bonfire with his family, and acknowledges that it contributes to the smoggy air. \n",
      "\n",
      "\u001b[31mTHIS IS THE TOKENIZED VERSION OF THE ENGLISH SENTENCE: \n",
      " \u001b[0m [31958 14160 10429     5     7 14414     5 14364   127 18953    33    17\n",
      "   209    63   820 27717   103 13502    13  5859  3971    30   186  1104\n",
      "     2     8  8409  3468 16393 20472     5    17    40 24398    33     9\n",
      "     4  8720  9538   105  1433     3     1     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0] \n",
      "\n",
      "\u001b[31mTHIS IS THE GERMAN TRANSLATION: \n",
      "\u001b[0m Hugo Miranda von Angel Caido schreibt, dass er sich immer daran erinnert ein Lagerfeuer mit seiner Familie angezündet zu haben und stimmt zu, dass es zu der verschmutzten Luft beiträgt. \n",
      "\n",
      "\u001b[31mTHIS IS THE TOKENIZED VERSION OF THE GERMAN TRANSLATION: \n",
      "\u001b[0m [31958 14160 10429     5    21 14414     5 14364   127 14774  4653     2\n",
      "    42    70    51   319  1430  6153    54 15985 23981    70    39   359\n",
      "  2338 30476 18847     5    18    89    12  5203    18     2    42    33\n",
      "    18    11 16450   177  8507 15553     3     1     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from termcolor import colored\n",
    "index = 10\n",
    "\n",
    "print(colored('THIS IS THE ENGLISH SENTENCE: \\n', 'red'), detokenize(input_batch[index], vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR), '\\n')\n",
    "print(colored('THIS IS THE TOKENIZED VERSION OF THE ENGLISH SENTENCE: \\n ', 'red'), input_batch[index], '\\n')\n",
    "print(colored('THIS IS THE GERMAN TRANSLATION: \\n', 'red'), detokenize(target_batch[index], vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR), '\\n')\n",
    "print(colored('THIS IS THE TOKENIZED VERSION OF THE GERMAN TRANSLATION: \\n', 'red'), target_batch[index], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de8dd00-8f12-4b40-b625-e5bf50e3460b",
   "metadata": {},
   "source": [
    "# NMT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e1ee4a8a-2ea1-4c4e-84e5-1ac02e7c65a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(inp_vocab_size, d_model, n_encode_layers):\n",
    "\n",
    "    inp_enc = tl.Serial(\n",
    "        tl.Embedding(vocab_size=inp_vocab_size, d_feature=d_model),\n",
    "        [tl.LSTM(n_units=d_model) for _ in range(n_encode_layers)]\n",
    "    )\n",
    "\n",
    "    return inp_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c69299ac-c050-41fd-accf-f61fc1ace013",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_attention_decoder(tar_vocab_size, d_model, mode):\n",
    "\n",
    "    tar_dec = tl.Serial(\n",
    "        tl.ShiftRight(mode=mode),\n",
    "        tl.Embedding(vocab_size=tar_vocab_size, d_feature=d_model),\n",
    "        tl.LSTM(n_units=d_model)\n",
    "    )\n",
    "\n",
    "    return tar_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ae0ace0b-25f2-45ff-bf0b-be0f002102cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_attention_input(enc_activations, dec_activations, inp):\n",
    "\n",
    "    keys = values = enc_activations\n",
    "\n",
    "    queries = dec_activations\n",
    "\n",
    "    mask = ~(inp == 0)\n",
    "\n",
    "    mask = fastnp.reshape(mask, (mask.shape[0], 1, 1, mask.shape[1]))\n",
    "\n",
    "    mask += fastnp.zeros((1, 1, dec_activations.shape[1], 1))\n",
    "\n",
    "    return queries, keys, values, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "2fafe921-d6c9-4bdd-b322-bea95816c3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NMTAttn(input_vocab_size=33300, target_vocab_size=33300, d_model=1024, n_encoder_layers=2, n_decoder_layers=2, n_attention_heads=4, attention_dropout=0.0, mode='train'):\n",
    "\n",
    "    enc_activations = encoder(input_vocab_size, d_model, n_encoder_layers)\n",
    "\n",
    "    tar_activations =  pre_attention_decoder(target_vocab_size, d_model, mode)\n",
    "\n",
    "    model = tl.Serial(\n",
    "        tl.Select([0,1,0,1]),\n",
    "        tl.Parallel(enc_activations, tar_activations),\n",
    "        tl.Fn('PrepareAttentionInput', prep_attention_input, n_out=4),\n",
    "        tl.Residual(tl.AttentionQKV(d_model, n_heads=n_attention_heads, dropout=attention_dropout, mode=mode)),\n",
    "        tl.Select([0,2]),\n",
    "        [tl.LSTM(n_units=d_model) for _ in range(n_decoder_layers)],\n",
    "        tl.Dense(target_vocab_size),\n",
    "        tl.LogSoftmax()\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "30283c2c-8853-4808-a660-f0c61731945a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serial_in2_out2[\n",
      "  Select[0,1,0,1]_in2_out4\n",
      "  Parallel_in2_out2[\n",
      "    Serial[\n",
      "      Embedding_33300_1024\n",
      "      LSTM_1024\n",
      "      LSTM_1024\n",
      "    ]\n",
      "    Serial[\n",
      "      Serial[\n",
      "        ShiftRight(1)\n",
      "      ]\n",
      "      Embedding_33300_1024\n",
      "      LSTM_1024\n",
      "    ]\n",
      "  ]\n",
      "  PrepareAttentionInput_in3_out4\n",
      "  Serial_in4_out2[\n",
      "    Branch_in4_out3[\n",
      "      None\n",
      "      Serial_in4_out2[\n",
      "        _in4_out4\n",
      "        Serial_in4_out2[\n",
      "          Parallel_in3_out3[\n",
      "            Dense_1024\n",
      "            Dense_1024\n",
      "            Dense_1024\n",
      "          ]\n",
      "          PureAttention_in4_out2\n",
      "          Dense_1024\n",
      "        ]\n",
      "        _in2_out2\n",
      "      ]\n",
      "    ]\n",
      "    Add_in2\n",
      "  ]\n",
      "  Select[0,2]_in3_out2\n",
      "  LSTM_1024\n",
      "  LSTM_1024\n",
      "  Dense_33300\n",
      "  LogSoftmax\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "model = NMTAttn()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "d08671ef-8ce7-42de-9ed3-4014592666ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_task_fn(x_train):\n",
    "\n",
    "    return training.TrainTask(\n",
    "        labeled_data=x_train,\n",
    "        loss_layer=tl.CrossEntropyLoss(),\n",
    "        optimizer=trax.optimizers.Adam(0.01),\n",
    "        lr_schedule=trax.lr.warmup_and_rsqrt_decay(1000, 0.1),\n",
    "        n_steps_per_checkpoint=100\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b592e614-69d4-4782-9786-cfdf57edd544",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_task = train_task_fn(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5b5b5606-53c5-45e7-8418-0a1ab5c753c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_task = training.EvalTask(\n",
    "    labeled_data=x_eval,\n",
    "    metrics=[tl.CrossEntropyLoss(), tl.Accuracy()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "0f41b02e-6162-41a9-9333-eae49f380c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/predator/miniconda3/envs/trax_environment/lib/python3.9/site-packages/jax/_src/xla_bridge.py:1183: UserWarning: jax.host_count has been renamed to jax.process_count. This alias will eventually be removed; please update your code.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will not write evaluation metrics, because output_dir is None.\n"
     ]
    }
   ],
   "source": [
    "training_loop = training.Loop(\n",
    "    NMTAttn(mode='train'),\n",
    "    train_task,\n",
    "    eval_tasks=[eval_task],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "02c09930-8259-4e32-918f-994b79e32ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training_loop.run(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5206489-b307-4a07-8265-86d368cb83ff",
   "metadata": {},
   "source": [
    "# EVAL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8714391-b031-472b-9e5d-9073b3aa0afa",
   "metadata": {},
   "source": [
    "### Greedy Decode Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d7cf6587-52c2-46b9-9f9c-7afb6322794b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NMTAttn(mode='eval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "6fa60980-5c17-48be-8cf9-46f362b490f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_symbol(NMTAttn, input_tokens, cur_output_tokens, temperature):\n",
    "\n",
    "    token_len = len(cur_output_tokens)\n",
    "\n",
    "    pad_length = 2 ** np.ceil(np.log2(token_len+1))\n",
    "\n",
    "    padded = cur_output_tokens + [0] * (pad_length - token_len)\n",
    "\n",
    "    pad_with_batch = np.reshape(padded, (1, pad_length))\n",
    "\n",
    "    outputs, _ = NMTAttn((input_tokens, pad_with_batch))\n",
    "\n",
    "    log_probs = output[0, token_len, :]\n",
    "\n",
    "    symbol = int(tl.logsoftmax_sample(log_probs, temperature))\n",
    "\n",
    "    return symbol, float(log_probs[symbol])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "35f3b5c0-b089-4729-a17b-19e0111307e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling_decode(input_sentence, NMTAttn = None, temperature=0.0, vocab_file=None, vocab_dir=None, next_symbol=next_symbol, tokenize=tokenize, detokenize=detokenize):\n",
    "\n",
    "    input_tokens = tokenize(input_sentence, vocab_file, vocab_dir)\n",
    "\n",
    "    cur_output_tokens = []\n",
    "\n",
    "    cur_output = 0\n",
    "\n",
    "    EOS = 1\n",
    "\n",
    "    while cur_output != EOS:\n",
    "\n",
    "        cur_output, log_prob = next_symbol(NMTAttn, input_tokens, cur_output_tokens, temperature)\n",
    "\n",
    "        cur_output_tokens.append(cur_output)\n",
    "\n",
    "    sentence = detokenize(cur_output_tokens, vocab_file=vocab_file, vocab_dir=vocab_dir)\n",
    "\n",
    "    return cur_output_tokens, log_prob, sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "21510bc4-f217-4dd5-94ff-7013ebdb41d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode_test(sentence, NMTAttn=None, vocab_file=None, vocab_dir=None, sampling_decode=sampling_decode, next_symbol=next_symbol, tokenize=tokenize, detokenize=detokenize):\n",
    "\n",
    "    _,_, sentence = sampling_decode(sentence, NMTAttn=NMTAttn, vocab_file=vocab_file, vocab_dir=vocab_dir, next_symbol=next_symbol, tokenize=tokenize, detokenize=detokenize)\n",
    "\n",
    "    print(\"English: \", sentence)\n",
    "    print(\"German: \", translated_sentence)\n",
    "\n",
    "    return translated_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "15fbe1ff-0db6-45dd-973e-e23fdb8290ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples(sentence, n_samples, NMTAttn=None, temperature=0.6, vocab_file=None, vocab_dir=None, sampling_decode=sampling_decode, next_symbol=next_symbol, tokenize=tokenize, detokenize=detokenize):\n",
    "\n",
    "    samples, log_probs = [], []\n",
    "\n",
    "    for _ in range(n_samples):\n",
    "        sample, logp, _ = sampling_decode(sentence, NMTAttn, temperature, vocab_file=vocab_file, vocab_dir=vocab_dir, next_symbol=next_symbol)\n",
    "\n",
    "        samples.append(sample)\n",
    "\n",
    "        log_probs.append(logp)\n",
    "\n",
    "    return samples, log_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7006222b-530e-4829-81e8-8a4592c304f8",
   "metadata": {},
   "source": [
    "### Similiarty / Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c703ebb4-71c0-429e-8ad5-5597439175c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(candidate, reference):\n",
    "\n",
    "    cand_set, ref_set = set(candidate), set(reference)\n",
    "\n",
    "    cand_ref_intersection = cand_set.intersection(ref_set)\n",
    "\n",
    "    cand_ref_union = cand_set.union(ref_set)\n",
    "\n",
    "    overlap = len(cand_ref_intersection) / len(cand_ref_union)\n",
    "\n",
    "    return overlap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a42abe-e0af-4007-91b1-960ea963b683",
   "metadata": {},
   "source": [
    "### Rouge Similarity / F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "ba213461-b8a6-415f-89f7-910f5f0d52c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def rouge1_similarity(system, reference):\n",
    "\n",
    "    sys_count = Counter(system)\n",
    "\n",
    "    ref_count = Counter(reference)\n",
    "\n",
    "    overlap = 0\n",
    "\n",
    "    for token in sys_count:\n",
    "\n",
    "        tok_count_sys = sys_count.get(token)\n",
    "\n",
    "        tok_count_ref = ref_count.get(token, 0)\n",
    "\n",
    "        overlap += np.minimum(tok_count_sys, tok_count_ref)\n",
    "\n",
    "    precision = overlap / sum(sys_count.values())\n",
    "    recall = overlap / sum(ref_count.values())\n",
    "\n",
    "    F1 = (2 * precision * recall) / (precision + recall)\n",
    "\n",
    "    return F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "bdea47c2-2e26-433a-8c12-0c5f3051323d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.8571428571428571)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge1_similarity([1, 2, 3], [1, 2, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "289f158d-fd14-4931-965e-f6a6ec90b3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_overlap(similarity_fn, samples, *ignore_params):\n",
    "\n",
    "    scores = {}\n",
    "\n",
    "    for i_can, can in enumerate(samples):\n",
    "\n",
    "        overlap = 0\n",
    "\n",
    "        for i_sam, sam in enumerate(samples):\n",
    "\n",
    "            if i_can == i_sam:\n",
    "                continue\n",
    "\n",
    "            overlap += similarity_fn(sam, can)\n",
    "\n",
    "        score = overlap / (len(samples) - 1)\n",
    "\n",
    "        scores[i_can] = score\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "d5f129be-7ead-4684-bad3-4c72d69e5f47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.45, 1: 0.625, 2: 0.575}"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_overlap(jaccard_similarity, [[1, 2, 3], [1, 2, 4], [1, 2, 4, 5]], [0.4, 0.2, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "5dd2dc32-43d0-45b9-bb49-8d6e6bfecc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_avg_overlap(similarity_fn, samples, log_probs):\n",
    "\n",
    "    scores = {}\n",
    "\n",
    "    for i_can, can in enumerate(samples):\n",
    "\n",
    "        overlap = 0\n",
    "        weighted_sum = 0\n",
    "\n",
    "        for i_sam, (sam, log_p) in enumerate(zip(samples, log_probs)):\n",
    "\n",
    "            if i_can == i_sam:\n",
    "                continue\n",
    "\n",
    "            s_overlap = similarity_fn(sam, can)\n",
    "\n",
    "            sample_p = float(np.exp(log_p))\n",
    "\n",
    "            weighted_sum += sample_p\n",
    "\n",
    "            overlap += s_overlap * sample_p\n",
    "\n",
    "        score = overlap / weighted_sum\n",
    "\n",
    "        scores[i_can] = score\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "82b8d4ad-03b8-49f0-b290-33e07f20f92e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.44255574831883415, 1: 0.631244796869735, 2: 0.5575581009406329}"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_avg_overlap(jaccard_similarity, [[1, 2, 3], [1, 2, 4], [1, 2, 4, 5]], [0.4, 0.2, 0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a104fa-bdcf-4463-81d3-52dd0b72974c",
   "metadata": {},
   "source": [
    "# Final Minimum bayes risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "4f2f7eb0-5a3b-4552-bb34-077c2adfac0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mbr_decode(sentence, n_samples, score_fn, similarity_fn, NMTAttn=None, temperature=0.6, vocab_file=None, vocab_dir=None, generate_samples=generate_samples, sampling_decode=sampling_decode, next_symbol=next_symbol, tokenize=tokenize, detokenize=detokenize):\n",
    "\n",
    "    samples, log_probs = generate_samples(sentence, n_samples, NMTAttn, temperature, vocab_file, vocab_dir)\n",
    "\n",
    "    scores = weighted_avg_overlap(similarity_fn, samples, log_probs)\n",
    "\n",
    "    max_score_key = max(scores, key=scores.get)\n",
    "\n",
    "    translated_sentence = detokenize(samples[max_score_key], vocab_file, vocab_dir)\n",
    "\n",
    "    return (translated_sentence, max_score_key, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1b85e6-f5cc-4a76-b7b3-092d1858c155",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
